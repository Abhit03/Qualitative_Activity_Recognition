---
title: "Qualitative Activity Recognition"
author: "Abhit"
date: "January 7th 2017"
output: html_document
---

##Summary
It is Scientificaly proven and a welll known fact that Physical activity leads to better health. There are many types of physical training to imrove different parts of the body. Weight training is one of the effective ways to improve cardio-respiratory fitness. A key requirement for effective training is proper technique. Incorrect technique can lead to training injuries like fractures and dislocations. We need a process for detecting incorrect executions i.e to recognise and categorise actvity based on how it is executed

In this assignment we explore Qualitative activtiy recognition. This scientific paper[1] defines a qualitative activity recognition system as a software artefact that observes the userâ€™s execution of an activity and compares it to a specification

Data used in this assignment was recorded from four sensors placed in the user's glove, armband, lumbar belt and dumbbell which provide three-axes acceleration, gyroscope and magnetometer data. Participants performed one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions labelled A, B, C, D and E Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

##Data Exploration
We explore the training data set which contains the weight lifting exercise data. 
The training dataset contains 19620 rows and 160 columns. We look at the columns and their summaries to extract the variables and features we use to train our model.

```{r, cache = TRUE, echo = FALSE}
dat <- read.csv('pml-training.csv')
tes <- read.csv('pml-testing.csv')
```

```{r, echo=FALSE, cache=TRUE}
vars <- c(
'roll_belt', 'pitch_belt', 'yaw_belt', 'total_accel_belt', 'gyros_belt_x',
'gyros_belt_y', 'gyros_belt_z', 'accel_belt_x', 'accel_belt_y', 'accel_belt_z',
'magnet_belt_x', 'magnet_belt_y', 'magnet_belt_z',

'roll_arm', 'pitch_arm', 'yaw_arm', 'total_accel_arm', 'gyros_arm_x',
'gyros_arm_y', 'gyros_arm_z', 'accel_arm_x', 'accel_arm_y', 'accel_arm_z',
'magnet_arm_x', 'magnet_arm_y', 'magnet_arm_z',

'roll_dumbbell', 'pitch_dumbbell', 'yaw_dumbbell', 'total_accel_dumbbell', 'gyros_dumbbell_x',
'gyros_dumbbell_y', 'gyros_dumbbell_z', 'accel_dumbbell_x', 'accel_dumbbell_y', 'accel_dumbbell_z',
'magnet_dumbbell_x', 'magnet_dumbbell_y', 'magnet_dumbbell_z',

'roll_forearm', 'pitch_forearm', 'yaw_forearm', 'total_accel_forearm', 'gyros_forearm_x',
'gyros_forearm_y', 'gyros_forearm_z', 'accel_forearm_x', 'accel_forearm_y', 'accel_forearm_z',
'magnet_forearm_x', 'magnet_forearm_y', 'magnet_forearm_z',

'classe' 
)

vars2 <- c(
'roll_belt', 'pitch_belt', 'yaw_belt', 'total_accel_belt', 'gyros_belt_x',
'gyros_belt_y', 'gyros_belt_z', 'accel_belt_x', 'accel_belt_y', 'accel_belt_z',
'magnet_belt_x', 'magnet_belt_y', 'magnet_belt_z',

'roll_arm', 'pitch_arm', 'yaw_arm', 'total_accel_arm', 'gyros_arm_x',
'gyros_arm_y', 'gyros_arm_z', 'accel_arm_x', 'accel_arm_y', 'accel_arm_z',
'magnet_arm_x', 'magnet_arm_y', 'magnet_arm_z',

'roll_dumbbell', 'pitch_dumbbell', 'yaw_dumbbell', 'total_accel_dumbbell', 'gyros_dumbbell_x',
'gyros_dumbbell_y', 'gyros_dumbbell_z', 'accel_dumbbell_x', 'accel_dumbbell_y', 'accel_dumbbell_z',
'magnet_dumbbell_x', 'magnet_dumbbell_y', 'magnet_dumbbell_z',

'roll_forearm', 'pitch_forearm', 'yaw_forearm', 'total_accel_forearm', 'gyros_forearm_x',
'gyros_forearm_y', 'gyros_forearm_z', 'accel_forearm_x', 'accel_forearm_y', 'accel_forearm_z',
'magnet_forearm_x', 'magnet_forearm_y', 'magnet_forearm_z'
)
```

We see that some of these columns contain NA's. A close examination reveals that they all contain same number of NAs i.e 19216 which is about 98% of the total number of rows. And these are all the rows which are **statistic of the some poistion coordinate (x, y, z) readings** like **mean, standard deviation, maximum, sum etc** as illustrated below. 
```{r}
summary(dat)[7, 27:31]
```

Now if we look at some of the direct sensor space coordinate readings they contain the data for entire rows. For ex - gyros_belt_x, gyros_belt_y, gyros_belt_z.

```{r}
summary(dat)[, 37:39]
```

We extract all the variables which have no missing data. They are all space or angle coordinate readings. There are 52 such variables in the dataset. 13 for each of these following sensors placed-
* Belt
* Arm
* Dumbell
* Forearm

An example is shown for Belt type -
```{r}
vars[1:13]
```

So we subset the data and select these specified variables. We proceed to selecting a model for training this data.

```{r, echo=FALSE}
dat <- dat[, vars]
tes <- tes[, vars2]
```

##Model Selection
The response variable is 'Classe' variable which is a factor variable with 5 levels. Lets consider the rest to be our predictors. **This is a classification problem**. 

* We did not use generalised linear models like logistic regression since that would be apt for binary classification. Here there are more than two levels
* A random forest could be used but since this is a large dataset it takes a lot of time to train the dataset and also dosent provide interpretability.
* We select boosting or **Gradient Boosting Method** as it trains the model relatively quickly and also gives good accuracy!

###Gradient Boosting Method
Gradient boosting is a machine learning technique for classification problems, which produces a prediction model in the form of an ensemble of decision trees. It builds the model in a stage-wise fashion generalizes them by allowing optimization of an arbitrary differentiable loss function.[2]

##Training the data
We use 'caret' package - A machine learning package for R. As it allows for easy slicing of data and training using prebuilt function method 'gbm' (gradient boosting method)

We slice the training data into a 70-30 split into testing and training dataset to perform **Cross validation** and evaluate our model. We can then estimate the estimated errors.

```{r, cache=TRUE}
library(caret)
set.seed(7)
inTrain <- createDataPartition(y=dat$classe,
                               p=0.7, list=FALSE)

training <- dat[inTrain,]
testing <- dat[-inTrain,]
```

We will save the model to an rda file since it takes a few minutes to train the algorithm
```{r, message=FALSE, warning=FALSE, eval = FALSE}
modFit <- train(classe ~ ., data=training, method="gbm", verbose = FALSE)
save(modFit, file = "gbm_model.rda")
```

Resampling results...
```{r, message=FALSE, warning=FALSE, cache=TRUE}
load("gbm_model.rda")
modFit 
```

##Error estimation
We can estimate the errors using the **Confusion matrix** and testing data. 
```{r, message=FALSE, warning=FALSE, cache=TRUE}
mtx = confusionMatrix(predict(modFit, testing), testing$classe)
mtx
```
* We get 96.74% accuracy with our testing part of the sliced dataset. We can proceed with predicting the given test case (in the other csv file) and determine **out of sample error**.

##Visualising the Confusion Matrix. 
```{r, message=FALSE, warning=FALSE, cache=TRUE}
z = data.frame(mtx$table)
p <- ggplot(data = z, aes(Reference, Prediction)) 
p <-  p + geom_tile(aes(fill = Freq), colour = "white") 
p <-  p + scale_fill_gradient(low = "lightblue", high = "steelblue")
p
```
* The plot illustrates the accuracy of the prediction for each class.

##Prediction on Test dataset
The test dataset contains 20 rows. we will apply our model and calculate for the accuracy of our prediction. Note that this data dosent contain the labels to test the accuracy by direct comparison.
```{r, eval=FALSE}
predict(modFit, test)
```
We get 19/20 predictions right with this model. (Which forms a part of the final assignment). That is 95% accuracy which **agrees with our error estimation**.

##Conclusion
We have used gradient boosting algorithm to train our dataset and classified the activities with 95% accuracy for the given test case.

##References
[1]<http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201>
[2]<http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf>
[3]<https://en.wikipedia.org/wiki/Gradient_boosting>
